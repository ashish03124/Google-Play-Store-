from google_play_scraper import app, reviews
import pandas as pd
import time

# ğŸš€ List of 40+ popular app package names across different categories
app_packages = [
    # Social Media
    'com.whatsapp', 'com.instagram.android', 'com.facebook.katana', 'com.snapchat.android', 'com.twitter.android',
    # Entertainment
    'com.netflix.mediaclient', 'com.google.android.youtube', 'com.spotify.music', 'com.mxtech.videoplayer.ad',
    # Shopping
    'com.flipkart.android', 'in.amazon.mShop.android.shopping', 'com.myntra.android', 'com.snapdeal.main', 'com.ajio.android',
    # Finance
    'com.phonepe.app', 'com.google.android.apps.nbu.paisa.user', 'com.mobikwik_new', 'com.paytm', 'com.indianbank.bc',
    # Education
    'com.byjus.thelearningapp', 'com.duolingo', 'co.learncbse.app', 'org.khanacademy.android', 'com.toppr.student',
    # Health & Fitness
    'com.curefit.curefit', 'cc.fitness.fitnesscoach', 'fit.betterme', 'com.healthifyme.basic', 'com.google.android.apps.fitness',
    # Games
    'com.tencent.ig', 'com.supercell.clashofclans', 'com.king.candycrushsaga', 'com.miniclip.eightballpool',
    'com.roblox.client', 'com.zhiliaoapp.musically',
    # Travel
    'com.makemytrip', 'com.tripadvisor.tripadvisor', 'com.ixigo.train.ixitrain', 'com.google.android.apps.maps', 'com.ola.customer'
]

# ğŸ—ƒï¸ Lists to store scraped data
all_app_data = []
all_reviews_data = []

# ğŸŒ€ Loop through each app
for idx, package in enumerate(app_packages):
    try:
        print(f"[{idx+1}/{len(app_packages)}] Fetching data for: {package}")
        
        # App Details
        app_info = app(package)
        app_info['package'] = package
        all_app_data.append(app_info)
        
        # App Reviews (limit to 200 for performance)
        app_reviews, _ = reviews(
            package,
            lang='en',
            country='us',
            count=200
        )
        for r in app_reviews:
            r['package'] = package
            all_reviews_data.append(r)

        time.sleep(1)  # to avoid rate limit

    except Exception as e:
        print(f"âš ï¸ Failed for {package}: {e}")
        continue

# ğŸ”„ Convert to DataFrames
df_apps = pd.DataFrame(all_app_data)
df_reviews = pd.DataFrame(all_reviews_data)

# ğŸ’¾ Save to CSV
df_apps.to_csv("apps_details_40.csv", index=False)
df_reviews.to_csv("apps_reviews_40.csv", index=False)

print("âœ… Scraping done! Data saved to apps_details_40.csv and apps_reviews_40.csv")
